{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSgFvHzhSPNK",
        "outputId": "a553d9f0-932e-4dde-ac88-1cfa6f0c8aa9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /Users/iris/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /Users/iris/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import urllib.request\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn import preprocessing\n",
        "import string\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mQ8JQ1scSabd"
      },
      "outputs": [],
      "source": [
        "# load the data\n",
        "non_clickbait_url = \"http://www.cs.columbia.edu/~sarahita/CL/non_clickbait_data.txt\"\n",
        "clickbait_url = \"http://www.cs.columbia.edu/~sarahita/CL/clickbait_data.txt\"\n",
        "\n",
        "# read url .txt file into string \"data\"\n",
        "def get_data(url):\n",
        "  data = urllib.request.urlopen(url).read().decode('utf-8')\n",
        "  return data\n",
        "\n",
        "non_clickbait_data = get_data(non_clickbait_url)\n",
        "clickbait_data = get_data(clickbait_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Hi_kgavhSdcv"
      },
      "outputs": [],
      "source": [
        "# combine clickbait and non-clickbait data in a single list\n",
        "non_clickbait_headlines = non_clickbait_data.rstrip('\\n').split('\\n')\n",
        "clickbait_headlines = clickbait_data.rstrip('\\n').split('\\n')\n",
        "all_headlines = non_clickbait_headlines + clickbait_headlines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "u4FcadFJSdXi"
      },
      "outputs": [],
      "source": [
        "# create a list of corresponding labels\n",
        "non_cb_labels = [0] * len(non_clickbait_headlines)\n",
        "cb_labels = [1] * len(clickbait_headlines)\n",
        "all_labels = non_cb_labels + cb_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cCeVdlJLORcv"
      },
      "outputs": [],
      "source": [
        "# for i in range(len(all_headlines)):\n",
        "#   return word_tokenize(all_headlines[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Ij2dh5X1PO71",
        "outputId": "e389b82e-845b-4d36-caea-463911ac33ff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Bill Changing Credit Card Rules Is Sent to Obama With Gun Measure Included'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_headlines[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-QHb6D8zSdNU"
      },
      "outputs": [],
      "source": [
        "# extract features: bag of stop words\n",
        "def stop_words(texts):\n",
        "        bow = []\n",
        "        eng_stopwords = stopwords.words('english')\n",
        "        for text in texts:      \n",
        "                counts = []\n",
        "                tokens = nltk.word_tokenize(text.lower())\n",
        "                for sw in eng_stopwords:\n",
        "                        sw_count = tokens.count(sw)\n",
        "                        counts.append(sw_count)\n",
        "                bow.append(counts)\n",
        "        bow_np = np.array(bow).astype(float)\n",
        "        return bow_np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wZaIA5CnSdCn"
      },
      "outputs": [],
      "source": [
        "# extract features\n",
        "stop_words_features = stop_words(all_headlines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yzb-5l6L04Qs",
        "outputId": "dea7604e-d448-453a-e43d-ca45e03d009d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(31998, 179)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stop_words_features.shape\n",
        "# headlines * stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6Or5IxUD37z",
        "outputId": "c46c3ce8-a364-420b-cea6-749ad81b0442"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "179\n"
          ]
        }
      ],
      "source": [
        "#stop_words_features[-2]\n",
        "print(len(stopwords.words('english')))\n",
        "\n",
        "# we have 179 stopwords list [ i, i+1, i+3...179]\n",
        "# we counter each item in the list each time we encounter that word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DygmPc5pScsf",
        "outputId": "c31e955f-d57e-4618-d785-118b5b977f00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.88       0.8790625  0.866875   0.878125   0.8815625  0.8821875\n",
            " 0.8834375  0.87125    0.8380744  0.87496093]\n",
            "0.8735535323538606\n"
          ]
        }
      ],
      "source": [
        "# convert features and labels to numpy arrays\n",
        "X = stop_words_features\n",
        "Y = np.array(all_labels) # target var to predict\n",
        "\n",
        "# run classifier using 10-fold cross validation\n",
        "# report mean accuracy \n",
        "\n",
        "scores = cross_val_score(MultinomialNB(), X, Y, scoring='accuracy', cv=10)\n",
        "print(scores)\n",
        "print(scores.mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVUr7eHcmxG0",
        "outputId": "0741331a-085e-4bb5-b1a3-8dc7f32615d8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /Users/iris/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "xLb-fTQ2m8r3"
      },
      "outputs": [],
      "source": [
        "# # extract features: bag of stop words\n",
        "# def my_pos_tags(texts):\n",
        "#         bow = []\n",
        "#         my_pos_tags_list = ['NN', 'NNP', 'DT', 'IN', 'JJ', 'NNS','CC','PRP','VB','VBG']\n",
        "#         for text in texts:      \n",
        "#                 counts = []\n",
        "#                 tokens = pos_tag(word_tokenize(text))\n",
        "#                 #print(tokens[0][1])\n",
        "#                 for pos in my_pos_tags_list:\n",
        "#                         #if my_pos_tag\n",
        "#                         pos_count = tokens.count(pos)\n",
        "#                         counts.append(pos_count)\n",
        "#                 bow.append(counts)\n",
        "#         pos_bow_np = np.array(bow).astype(float)\n",
        "#         return pos_bow_np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "78nE2s72CXm0"
      },
      "outputs": [],
      "source": [
        "sample_train = all_headlines[2:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "dHWvTjl4IqcQ"
      },
      "outputs": [],
      "source": [
        "# extract features: bag of stop words\n",
        "def my_pos_tags(texts):\n",
        "        bow = []\n",
        "        my_pos_tags_list = ['NN', 'NNP', 'DT', 'IN', 'JJ', 'NNS','CC','PRP','VB','VBG']\n",
        "        for text in texts:      \n",
        "                counts = []\n",
        "                tokens = pos_tag(word_tokenize(text))\n",
        "                #print(tokens[0][1])\n",
        "                for pos in my_pos_tags_list:\n",
        "                        #if my_pos_tag\n",
        "                        pos_count = tokens[0][1].count(pos)\n",
        "                        counts.append(pos_count)\n",
        "                bow.append(counts)\n",
        "        pos_bow_np = np.array(bow).astype(float)\n",
        "        return pos_bow_np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 268,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "JSn1ixKtK0ol",
        "outputId": "cc48e02b-ee27-45fb-cf07-d47a78ebec58"
      },
      "outputs": [],
      "source": [
        "def my_pos(texts):\n",
        "        bow = []\n",
        "        pos_tags_list = ['NN', 'NNP', 'DT', 'IN', 'JJ', 'NNS','CC','PRP','VB','VBG']\n",
        "        \n",
        "        for text in texts:\n",
        "                counts = []\n",
        "                #tokens = [word_tokenize(i) for i in texts]\n",
        "                tokens = word_tokenize(text)\n",
        "                #print(tokens)\n",
        "        \n",
        "                tagged_tokens = pos_tag(tokens)\n",
        "                #tagged_tokens = [pos_tag(j) for j in tokens]\n",
        "                #print(tagged_tokens)\n",
        "\n",
        "                only_token = [tag[1] for tag in tagged_tokens]\n",
        "                #print(only_token)\n",
        "\n",
        "                for pos in pos_tags_list:\n",
        "                        tokens_count = only_token.count(pos)\n",
        "                        counts.append(tokens_count)\n",
        "                bow.append(counts)\n",
        "\n",
        "        bow_arr = np.array(bow).astype(float)\n",
        "        return bow_arr\n",
        "\n",
        "# AttributeError: 'list' object has no attribute 'isdigit'\n",
        "# you applied this to the list. you shoudl have applied to the string inside the list. \n",
        "# e.g. 'abc'.isdigit() \n",
        "#       [].isdigit() // error\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 280,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdIMfdzK6tHK",
        "outputId": "c78a7673-4d34-45b6-ade8-28a894d24a7c"
      },
      "outputs": [],
      "source": [
        "pos_feature = my_pos(all_headlines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 281,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(31998, 10)"
            ]
          },
          "execution_count": 281,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pos_feature.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 282,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.763125   0.760625   0.748125   0.758125   0.76       0.7740625\n",
            " 0.750625   0.7546875  0.7561738  0.75273523]\n",
            "0.7578284034073148\n"
          ]
        }
      ],
      "source": [
        "# convert features and labels to numpy arrays\n",
        "X_sync = pos_feature\n",
        "Y_sync = np.array(all_labels) # target var to predict\n",
        "\n",
        "# run classifier using 10-fold cross validation\n",
        "# report mean accuracy \n",
        "\n",
        "pos_scores = cross_val_score(MultinomialNB(), X_sync, Y_sync, scoring='accuracy', cv=10)\n",
        "print(pos_scores)\n",
        "print(pos_scores.mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#https://kavita-ganesan.com/how-to-use-countvectorizer/\n",
        "def my_lexicon(texts):\n",
        "        bow = []\n",
        "        pos_tags_list = ['NN', 'NNP', 'DT', 'IN', 'JJ', 'NNS','CC','PRP','VB','VBG']\n",
        "        \n",
        "        for text in texts:\n",
        "                counts = []\n",
        "                #tokens = [word_tokenize(i) for i in texts]\n",
        "                tokens = word_tokenize(text)\n",
        "                #print(tokens)\n",
        "        \n",
        "                tagged_tokens = pos_tag(tokens)\n",
        "                #tagged_tokens = [pos_tag(j) for j in tokens]\n",
        "                #print(tagged_tokens)\n",
        "\n",
        "                only_token = [tag[1] for tag in tagged_tokens]\n",
        "                #print(only_token)\n",
        "\n",
        "                for pos in pos_tags_list:\n",
        "                        tokens_count = only_token.count(pos)\n",
        "                        counts.append(tokens_count)\n",
        "                bow.append(counts)\n",
        "\n",
        "        bow_arr = np.array(bow).astype(float)\n",
        "        return bow_arr\n",
        "\n",
        "# AttributeError: 'list' object has no attribute 'isdigit'\n",
        "# you applied this to the list. you shoudl have applied to the string inside the list. \n",
        "# e.g. 'abc'.isdigit() \n",
        "#       [].isdigit() // error\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "e04e036266f305a312129840b06cfb353c1d2d005428a9f350cd8fe1c5454b5e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
